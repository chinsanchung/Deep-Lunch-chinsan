{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 모두의 딥러닝 셋째 마당 '딥러닝의 시작, 신경망'\n",
        "\n",
        "## 7장. 퍼셉트론과 인공지능의 시작\n",
        "\n",
        "### 1. 인공지능의 시작을 알린 퍼셉트론\n",
        "\n",
        "- 인간의 뉴런을 통한 신호 전달 방식을 모방하고자 만든 장치가 **퍼셉트론**입니다.\n",
        "    - 입력 값을 여러 개를 받아 출력 값을 만드는데, 입력 값에 가중치를 조절하여 학습을 할 수 있도록 했습니다.\n",
        "- 그 외에도 **아달라인**이라는 장치도 나왔는데, 서포트 벡터 머신 등 머신 러닝의 중요한 알고리즘으로 발전했습니다.\n",
        "\n",
        "### 2. 퍼셉트론의 과제 + 3. XOR 문제\n",
        "\n",
        "퍼셉트론에는 2차원 평면 상에 직선을 긋는 것만 가능하다는 한계가 있었습니다. 그래서 XOR 문제조차 풀 수가 없었는데, 다층 퍼셉트론과 오차 역전파를 개발하면서 해결하게 됩니다.\n",
        "\n",
        "|x1|x2|결과|\n",
        "|--|--|--|\n",
        "|0|0|0|\n",
        "|0|1|1|\n",
        "|1|0|1|\n",
        "|1|1|0|\n",
        "\n",
        "## 8장. 다층 퍼셉트론\n",
        "\n",
        "### 1. 다층 퍼셉트론의 등장\n",
        "\n",
        "퍼셉트론으로 XOR 문제를 풀기 위해 나온 개념이 '퍼셉트론 두 개를 한 번에 계산'하는 것입니다. 그러기 위해선 퍼셉트론을 각각 처리할 은닉층을 만들 필요가 있습니다.\n",
        "\n",
        "### 2. 다층 퍼셉트론의 설계\n",
        "\n",
        "- 입력 값 $x_1$, $x_2$에 가중치 $w$를 곱하고 바이어스 $b$를 더해 은닉층으로 전송합니다.\n",
        "    - 은닉층의 중간 정거장을 노드라 부릅니다. ($n_1$, $n_2$으로 표기)\n",
        "- 은닉층에서 취합한 값은 활성화 함수를 통해 다음 단계로 전송합니다.\n",
        "- 이제 노드에서의 결과를 출력층으로 전송한 후, 출력층에서 다시 활성화 함수를 통해 예측 값 $y$를 계산합니다. ($y_{out}$으로 표기)\n",
        "\n",
        "$$n_1 = σ (x_1 w_11 + x_2 w_21 + b_1)$$ \n",
        "$$n_2 = σ (x_1 w_12 + x_2 w_22 + b_2)$$\n",
        "$$y_{out} = σ (n_1 w_31 + n_2 w_32 + b_3)$$\n",
        "\n",
        "여기서는 가중치 여섯 개와 바이어스 세 개가 필요합니다.\n",
        "\n",
        "---\n",
        "\n",
        "다층 퍼셉트론으로 XOR 문제를 해결할 수 있었지만, 은닉층의 가중치를 학습하는 방법을 찾기까지 20여 년의 시간이 필요했습니다.(인공지능의 겨울) \n",
        "\n",
        "제프리 힌튼 교수의 여러 아이디어로 겨울을 극복했는데, 그 중 하나가 **오차 역전파**입니다."
      ],
      "metadata": {
        "id": "gkHhE1J5P4gJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9장. 오차 역전파에서 딥러닝으로\n",
        "\n",
        "### 1. 딥러닝의 태동, 오차 역전파\n",
        "\n",
        "경사 하강법으로 임의의 가중치를 선언해 오차를 구하고, 오차가 최소가 될 때까지 반복하는 과정을 거치는 방법은 입력층과 출력층만 존재할 때 사용할 수 있는 방법입니다.\n",
        "\n",
        "은닉층이 생기면서 2번의 경사 하강법을 실행해야 하는데, 문제는 **은닉층은 겉으로 드러나지 않으므로 오차를 구할 만한 적절한 출력 값이 없어서** 2번째 경사 하강법을 실행하기 어렵다는 것입니다.\n",
        "\n",
        "그래서 출력층의 오차를 다시 한 번 사용해서 문제를 해결합니다. 즉 오차 2개를 모두 계산해서 편미분합니다. 아래의 공식을 델타식이라 부르며, 이를 이용해 은닉층의 숫자가 늘어도 계산할 수 있게 됩니다.\n",
        "\n",
        "- 첫 번째 가중치 업데이트 공식 = $(y_{o1} - y_{실제값}) * y_{o1}(1 - y_{o1}) * y_{h1}$\n",
        "- 두 번째 가중치 업데이트 공식 = $(δy_{o1} * w_{31} + δy_{o2} * w_{41}) * y_{h1}(1 - y_{h1}) * x_1$\n",
        "\n",
        "### 2. 활성화 함수와 고급 경사 하강법\n",
        "\n",
        "하지만 위의 역전파에는 문제가 있습니다. 깊은 층을 계속 만들 경우, 출력층에서 시작한 가중치의 갱신이 점차 0에 가까워지는 문제가 생깁니다. 이는 시그모이드 함수의 특성 때문으로, 1보다 작은 시그모이드를 미분한 값(최대 0.25)를 계속 곱하다 보니 기울기가 점차 사라지게 되는 것입니다.\n",
        "\n",
        "이를 해결하고자 나온 것이 **렐루(ReLU)**라는 활성화 함수입니다.\n",
        "\n",
        "- 시그모이드 함수: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
        "- 렐루 함수: $f(x) = x (x > 0)$ 또는 $f(x) = 0 (x <= 0)$\n",
        "\n",
        "렐루 함수를 미분하면 x 값이 0보다 클 경우 1이 됩니다. 그 결과 여러 번 오차 역전파를 진행해도 맨 처음 층까지 값을 유지할 수 있게 되었습니다.\n",
        "\n",
        "그 외에도 하이퍼볼릭 탄젠트(hyperbolic tangent), 소프트플러스(softplus) 함수 등 여러 활성화 함수가 등장했습니다.\n",
        "\n",
        "### 3. 속도와 정확도 문제를 해결하는 고급 경사 하강법\n",
        "\n",
        "경사 하강법은 계산량이 매우 많다는 단점이 있어서, 그것을 보완하는 여러 고급 경사 하강법이 등장했습니다.\n",
        "\n",
        "#### 확률적 경사 하강법(Gradient Descent)\n",
        "\n",
        "한 번 업데이트할 때마다 전체 데이터를 미분하는 대신, 랜덤하게 추출한 일부 데이터로만 미분을 진행하는 방법입니다. 그래서 작업 속도를 높이고 보다 여러 번 업데이트를 할 수 있습니다.\n",
        "\n",
        "랜덤한 일부 데이터를 사용하기에 중간 결과의 진폭이 크고 불안정해 보이지만, 속도가 확연히 빠르며 최적 해에 근사한 값을 찾아낸다는 장점이 있습니다.\n",
        "\n",
        "#### 모멘텀(momentum)\n",
        "\n",
        "경사 하강법처럼 매번 기울기를 구하는 것은 같지만, 오차를 수정하기 전에 바로 앞에서의 값와 + - 방향을 참고해 일정한 비율만 수정하는 방법입니다. 따라서 수정 방향이 + - 로 지그재그로 일어나는 현상이 줄고, 관성 효과를 얻을 수 있습니다.\n",
        "\n",
        "---\n",
        "\n",
        "그 외에도 여러 방법이 나왔는데 지금은 정확도와 속도를 모두 향상시킨 **아담(adam)**이라는 고급 경사 하강법을 주로 사용하고 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fP1b-Vp2TZrD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JI_qowpkUqrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}